{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xMPPzrjus37"
   },
   "source": [
    "El Proyecto PyTorch contiene librerías para diferentes tipos de datos y fines.\n",
    "\n",
    "* `torchaudio`\n",
    "* `torchvision`\n",
    "* `TorchElastic`\n",
    "* `TorchServe`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edfy_16KwDse"
   },
   "source": [
    "Vamos a utilizar `torchtext` para clasificación de texto. El paquete `torchtext` consta de utilidades de procesamiento de datos y conjuntos de datos populares para lenguaje natural.\n",
    "\n",
    "Sin embargo, no dudes en probar otras de las librerías disponibles en PyTorch. ¡`torchvision` es particularmente utilizado por aplicaciones que trabajan con imágenes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMU-JipBtNZP"
   },
   "source": [
    "## 1. Importando librerías y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G5Ms82uBZmTT"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install portalocker>=2.0.0\n",
    "!pip install torchtext --upgrade\n",
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T9XsrhAbZqJR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.0+cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "# from torchtext.datasets import DBpedia\n",
    "from utils.dbpedia import DBpedia\n",
    " \n",
    "# Comprobar la versión\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pk7nYHCpmD6H"
   },
   "source": [
    "## 2. Procesando el dataset y creando un vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qILmEFSzN7XN"
   },
   "source": [
    "Importa las bibliotecas `torch` y `torchtext`. Utiliza `torchtext` para cargar el conjunto de datos DBpedia. \n",
    "\n",
    "Luego, utiliza la función `iter` para crear un objeto de iteración para el conjunto de datos de entrenamiento. Finalmente, el código imprime la versión de la biblioteca `torchtext` utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "roB407LAU4mQ"
   },
   "outputs": [],
   "source": [
    "train_iter = iter(DBpedia(split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UlY5TiJvnnL7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Marvell Software Solutions Israel  Marvell Software Solutions Israel known as RADLAN Computer Communications Limited before 2007 is a wholly owned subsidiary of Marvell Technology Group that specializes in local area network (LAN) technologies.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L1JZNj4wXu6"
   },
   "source": [
    "Construiremos un vocabulario con el conjunto de datos implementando la función incorporada `build_vocab_from_iterator`que acepta el iterador que produce una lista o un iterador de tokens.\n",
    "\n",
    "Usamos `torchtext` para construir un vocabulario a partir de un conjunto de datos del DBpedia en inglés. \n",
    "\n",
    "En primer lugar, importa la función `get_tokenizer` de la biblioteca `torchtext` para obtener un tokenizador predefinido para el idioma inglés. Luego, define un iterador de datos para el conjunto de datos de entrenamiento de DBpedia.\n",
    "\n",
    "A continuación, se define una función `yield_tokens` que utiliza el tokenizador para dividir el texto en tokens y devolverlos uno a uno. Esta función se utiliza como entrada para la función `build_vocab_from_iterator`, que construye un vocabulario a partir de los tokens devueltos por la función `yield_tokens`. La función `build_vocab_from_iterator` también toma una lista de tokens especiales, que se utilizarán para representar palabras fuera del vocabulario.\n",
    "\n",
    "Finalmente, se establece el índice predeterminado del vocabulario en el token \"<unk>\", que se utiliza para representar palabras que no están presentes en el vocabulario. En resumen, este fragmento de código construye un vocabulario a partir de un conjunto de datos de entrenamiento y lo prepara para su uso en modelos de aprendizaje automático que utilizan PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kzSZNEbakkut"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizador = get_tokenizer(\"basic_english\")\n",
    "train_iter = DBpedia(split=\"train\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, texto in data_iter:\n",
    "        yield tokenizador(texto)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICy2-WqrxR1S"
   },
   "source": [
    "Nuestro vocabulario transforma la lista de tokens en números enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "51YhJeT3n30n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7296, 1506, 47, 578, 2323, 187, 2409, 5, 0, 1078]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(tokenizador(\"Hello how are you? I am a platzi student\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4zd0buNx3QU"
   },
   "source": [
    "Definimos dos funciones lambda, `text_pipeline` y `label_pipeline`, que se utilizan para procesar los datos de entrada en un formato que se puede utilizar para entrenar y evaluar modelos.\n",
    "\n",
    "La primera función, `text_pipeline`, toma una cadena de texto como entrada y la procesa utilizando el tokenizador y el vocabulario que definimos. Recuerda que el tokenizador divide el texto en tokens (palabras o subpalabras), mientras que el vocabulario mapea cada token a un índice entero único. La función devuelve una lista de índices enteros que representan los tokens en el texto.\n",
    "\n",
    "La segunda función, `label_pipeline`, toma una etiqueta como entrada y la convierte en un número entero. En este caso, la etiqueta se resta en `1` para ajustarla a un rango de índice de `0` a `n-1`, donde `n` es el número de clases en el problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7tp-PHphn522"
   },
   "outputs": [],
   "source": [
    "texto_pipeline = lambda x: vocab(tokenizador(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uL_T7yz6n8al"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7296, 187, 2409, 2159]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_pipeline(\"Hello i am Luis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-EhktKnRn9tq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline(\"10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT1kUQ-HydvN"
   },
   "source": [
    "\n",
    "Creamos una función llamada `collate_batch` para procesar un lote de datos. La entrada batch es una lista de tuplas, donde cada tupla contiene una etiqueta y su correspondiente texto.\n",
    "\n",
    "* Se inicializan tres listas: `label_list`, `text_list` y `offsets`. Offsets almacena el índice de inicio de cada secuencia de texto en el tensor concatenado de secuencias de texto. Ayuda a realizar un seguimiento de los límites de las secuencias de texto individuales dentro del tensor concatenado. Comienza con un valor 0, que representa el índice de inicio de la primera secuencia de texto.\n",
    "\n",
    "* La función recorre cada punto de datos en el lote. Para cada punto de datos, procesa la etiqueta utilizando `label_pipeline(_label)` y agrega el resultado a `label_list`. Procesa el texto utilizando `texto_pipeline(_text)` y lo convierte en un tensor de tipo torch.`int64`. El texto procesado se agrega a `text_list` y su longitud `(size(0))` se agrega a offsets.\n",
    "\n",
    "* El último elemento en la lista offsets se elimina mediante el corte `offsets[:-1]`. Luego, la función `cumsum` calcula la suma acumulativa de los elementos en la lista offsets a lo largo de la dimensión 0.\n",
    "\n",
    "* La `text_list` se concatena en un único tensor 1D utilizando `torch.cat(text_list)`.\n",
    "\n",
    "* Los tensores `label_list`, `text_list` y `offsets` se convierten al dispositivo especificado (ya sea GPU o CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HXM1uoT8n_y0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "    offsets = [0]\n",
    "\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(texto_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un `DataLoader` maneja el proceso de iteración a través de un conjunto de datos en mini lotes. El DataLoader es importante porque ayuda a administrar de manera eficiente la memoria, mezclar los datos y paralelizar fácilmente la carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iter = DBpedia(split=\"train\")\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f633ccc9ff0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Zxp4wbmVce"
   },
   "source": [
    "## 3. Creación de modelo de clasificación y sus capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF_m53Ab0oiK"
   },
   "source": [
    "Definimos una clase que hereda `nn.Module` y que representa un modelo de clasificación de texto utilizando las capas de `EmbeddingBag` y `Linear`. El modelo toma como entrada el tamaño del vocabulario, la dimensión del embedding y el número de clases. Luego define la estructura del modelo utilizando las capas mencionadas anteriormente.\n",
    "\n",
    "La función `forward` del modelo toma como entrada el texto y los desplazamientos correspondientes (offsets), que se utilizan para descomponer el texto en lotes (batches). La función `EmbeddingBag` se utiliza para transformar el texto en una representación de embedding. La capa `Lineal` se utiliza para realizar la clasificación de texto.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "35MlAHmY_IVm"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModeloClasificacionTexto(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(ModeloClasificacionTexto, self).__init__()\n",
    "\n",
    "        # Capa de incrustación (embedding)\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        \n",
    "        # Capa de normalización por lotes (batch normalization)\n",
    "        self.bn1 = nn.BatchNorm1d(embed_dim)\n",
    "        \n",
    "        # Capa completamente conectada (fully connected)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Incrustar el texto (embed the text)\n",
    "        embedded = self.embedding(text, offsets)\n",
    "\n",
    "        # Aplicar la normalización por lotes (apply batch normalization)\n",
    "        embedded_norm = self.bn1(embedded)\n",
    "\n",
    "        # Aplicar la función de activación ReLU (apply the ReLU activation function)\n",
    "        embedded_activated = F.relu(embedded_norm)\n",
    "\n",
    "        # Devolver las probabilidades de clase (output the class probabilities)\n",
    "        return self.fc(embedded_activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZFru1zL1soL"
   },
   "source": [
    "Construimos un modelo con una dimensión de embedding de 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "d2KLlcb8oE7s"
   },
   "outputs": [],
   "source": [
    "train_iter = DBpedia(split=\"train\")\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100\n",
    "\n",
    "modelo = ModeloClasificacionTexto(vocab_size=vocab_size, embed_dim=embedding_size, num_class=num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802998"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6Yf7_o-h83XL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 80,301,414 parámetros entrenables\n"
     ]
    }
   ],
   "source": [
    "# arquitectura\n",
    "# print(modelo)\n",
    "\n",
    "# Número de parámetros entrenables en nuestro modelo\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"El modelo tiene {count_parameters(modelo):,} parámetros entrenables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9erCn5fmiU_"
   },
   "source": [
    "## 4. Funciones para entrenamiento y evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx_CnCPv3ZC4"
   },
   "source": [
    "Ahora, definimos las funciones para entrenar el modelo y evaluar los resultados.\n",
    "\n",
    "Utilizamos `torch.nn.utils.clip_grad_norm_` para limitar el valor máximo de la norma del gradiente durante el entrenamiento de una red neuronal. En otras palabras, se asegura de que los gradientes no sean demasiado grandes y, por lo tanto, evita que la red neuronal se vuelva inestable durante el entrenamiento.\n",
    "\n",
    "El primer argumento, `modelo.parameters()`, se refiere a los parámetros del modelo que se están entrenando. El segundo argumento, \"0.1\", es el valor máximo permitido para la norma del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NgXcW2GroHJ1"
   },
   "outputs": [],
   "source": [
    "def entrena(dataloader):\n",
    "    # Colocar el modelo en formato de entrenamiento\n",
    "    modelo.train()\n",
    "\n",
    "    # Inicializa accuracy, count y loss para cada epoch\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    total_count = 0 \n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        # reestablece los gradientes después de cada batch\n",
    "        optimizer.zero_grad()\n",
    "        # Obten predicciones del modelo\n",
    "        prediccion = modelo(text, offsets)\n",
    "\n",
    "        # Obten la pérdida\n",
    "        loss = criterio(prediccion, label)\n",
    "        \n",
    "        # backpropage la pérdida y calcular los gradientes\n",
    "        loss.backward()\n",
    "        \n",
    "        # Obten la accuracy\n",
    "        acc = (prediccion.argmax(1) == label).sum()\n",
    "        \n",
    "        # Evita que los gradientes sean demasiado grandes \n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1)\n",
    "\n",
    "        # Actualiza los pesos\n",
    "        optimizer.step()\n",
    "\n",
    "        # Llevamos el conteo de la pérdida y el accuracy para esta epoch\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "        if idx % 500 == 0 and idx > 0:\n",
    "          print(f\" epoca {epoch} | {idx}/{len(dataloader)} batches | perdida {epoch_loss/total_count} | accuracy {epoch_acc/total_count}\")\n",
    "\n",
    "    return epoch_acc/total_count, epoch_loss/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "p7CrDdiBCfGa"
   },
   "outputs": [],
   "source": [
    "def evalua(dataloader):\n",
    "    modelo.eval()\n",
    "    epoch_acc = 0\n",
    "    total_count = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            \n",
    "            # Obtenemos la la etiqueta predecida\n",
    "            prediccion = modelo(text, offsets)\n",
    "\n",
    "            # Obtenemos pérdida y accuracy\n",
    "            loss = criterio(prediccion, label)\n",
    "            acc = (prediccion.argmax(1) == label).sum()\n",
    "\n",
    "            # Llevamos el conteo de la pérdida y el accuracy para esta epoch\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return epoch_acc/total_count, epoch_loss/total_count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVPSvNQMmsP1"
   },
   "source": [
    "## 5. Preparando el entrenamiento: split de datos, pérdida y optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUx3ZTzN3-oc"
   },
   "source": [
    "Dividimos el conjunto de datos de entrenamiento en conjuntos de entrenamiento válidos con una proporción de división de 0.95 (entrenamiento) y 0.05 (válido) utilizando la función `torch.utils.data.dataset.random_split`\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "rE9Bq2O-XNq_"
   },
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "\n",
    "EPOCHS = 4 # epochs\n",
    "TASA_APRENDIZAJE = 0.2  # tasa de aprendizaje\n",
    "BATCH_TAMANO = 64 # tamaño de los batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu-TQyvd5KMI"
   },
   "source": [
    "Explora las otras funciones de pérdida disponibles en PyTorch. Puedes encontrarlas todas aquí: https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
    "\n",
    "La función de pérdida es la que mide qué tan buenas son las predicciones de nuestro modelo en comparación con las etiquetas reales. PyTorch ofrece una amplia gama de funciones de pérdida que podemos utilizar para entrenar nuestros modelos en diferentes tipos de problemas, como regresión, clasificación y modelado de secuencia a secuencia.\n",
    "\n",
    "Al profundizar en estas otras funciones de pérdida, podemos ampliar nuestro conocimiento de machine learning. Lo mismo aplica para los optimizadores. PyTorch proporciona una variedad de algoritmos de optimización: https://pytorch.org/docs/stable/optim.html#algorithms.\n",
    "\n",
    "Dedica tiempo a explorar la documentación de PyTorch sobre funciones de pérdida y optimizadores. Experimenta con diferentes funciones en tus proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DtM-yw0QX5ac"
   },
   "outputs": [],
   "source": [
    "# Pérdida, optimizador\n",
    "criterio = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr= TASA_APRENDIZAJE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vas4IcYu7qV1"
   },
   "source": [
    "Dividimos el conjunto de datos en tres partes: entrenamiento, validación y prueba. \n",
    "\n",
    "Primero, importamos la función `random_split` de la clase Dataset y la función `to_map_style_dataset` de `torchtext.data.functional`. Luego, cargamos el conjunto de datos `DBpedia` usando el método `DBpedia()`. A continuación, convertimos el conjunto de datos en un formato que pueda ser utilizado por el `DataLoader` de PyTorch utilizando la función `to_map_style_dataset`.\n",
    "\n",
    "Luego, definimos la proporción de datos que utilizaremos para entrenar nuestro modelo (el 95%) y el porcentaje que utilizaremos para validar nuestro modelo (el 5%). Utilizamos la función `random_split` para dividir el conjunto de datos de entrenamiento en entrenamiento y validación.\n",
    "\n",
    "Finalmente, creamos tres DataLoaders para cada parte del conjunto de datos: uno para el entrenamiento, uno para la validación y otro para la prueba. Utilizamos el argumento `batch_size` para definir el tamaño de los lotes de datos que se utilizarán en el entrenamiento y la prueba. El argumento `collate_fn` especifica cómo se deben unir las muestras de datos para formar un lote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Er7axMVCZw5r"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Obten el trainset y testset\n",
    "train_iter, test_iter = DBpedia()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Entrenamos el modelo con el 95% de los datos del trainset\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Creamos un dataset de validación con el 5% del trainset\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
    "\n",
    "# Creamos dataloaders listos para ingresar a nuestro modelo\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_TAMANO, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_TAMANO, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_TAMANO, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gliGM0f8m41V"
   },
   "source": [
    "## 6. Entrenamiento y evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL8OJQFp8IfR"
   },
   "source": [
    "Ahora vamos a entrenar y evaluar nuestro modelo. En primer lugar, se define la variable `mejor_loss_validacion` y se inicializa con un valor infinito positivo. Esta variable se utiliza para realizar un seguimiento de la mejor pérdida de validación durante el entrenamiento.\n",
    "\n",
    "Luego, se realiza un `for` a través de las épocas. Dentro de cada época, se realiza el entrenamiento y la validación del modelo utilizando los conjuntos de datos de entrenamiento y validación respectivamente.\n",
    "\n",
    "En otras palabras, si la pérdida de validación actual es menor que la mejor pérdida de validación anterior, se guarda el estado actual del modelo en el archivo `pesos_guardados.pt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qls71GpHoLfH"
   },
   "outputs": [],
   "source": [
    "# Obten la mejor pérdida \n",
    "\n",
    "\n",
    "# Entrenamos\n",
    "\n",
    "    # Entrenamiento\n",
    "    \n",
    "    # Validación\n",
    "    \n",
    "\n",
    "    # Guarda el mejor modelo\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us9eYmrT4YFq"
   },
   "source": [
    "Evaluamos el modelo en el test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-rMpCWWoObX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f'Accuracy del test dataset -> ')\n",
    "print(f'Pérdida del test dataset -> ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WE1DDsfm-1-"
   },
   "source": [
    "## 7. Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ee1sonK5Dns"
   },
   "source": [
    "Probemos con un ejemplo aleatorio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98utU4rmoRQN"
   },
   "outputs": [],
   "source": [
    "DBpedia_label = {1: 'Company',\n",
    "                2: 'EducationalInstitution',\n",
    "                3: 'Artist',\n",
    "                4: 'Athlete',\n",
    "                5: 'OfficeHolder',\n",
    "                6: 'MeanOfTransportation',\n",
    "                7: 'Building',\n",
    "                8: 'NaturalPlace',\n",
    "                9: 'Village',\n",
    "                10: 'Animal',\n",
    "                11: 'Plant',\n",
    "                12: 'Album',\n",
    "                13: 'Film',\n",
    "                14: 'WrittenWork'}\n",
    "\n",
    "\n",
    "ejemplo_1 = \"Nithari is a village in the western part of the state of Uttar Pradesh India bordering on New Delhi. Nithari forms part of the New Okhla Industrial Development Authority's planned industrial city Noida falling in Sector 31. Nithari made international news headlines in December 2006 when the skeletons of a number of apparently murdered women and children were unearthed in the village.\"\n",
    "ejemplo_2 = \"Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"El ejemeplo 1 es una texto de categoría {}\")\n",
    "print(f\"El ejemeplo 2 es una texto de categoría {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ILMPS41nBWI"
   },
   "source": [
    "## 8. Almacenamiento y carga del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4lhw1sIEBuq"
   },
   "source": [
    "El método `state_dict()` se utiliza para devolver el diccionario del estado del modelo. Este diccionario contiene todos los parámetros entrenables del modelo. Como pesos y sesgos en forma de tensores de PyTorch.\n",
    "\n",
    "Es útil para una variedad de tareas, como guardar y cargar modelos o transferir los parámetros aprendidos de un modelo a otro. Permite manipular fácilmente el estado del modelo como un diccionario de parámetros con nombres, sin tener que acceder a ellos directamente.\n",
    "\n",
    "Por ejemplo, si queremos guardar nuestro modelo en el disco de memoria, podemos utilizarlo para obtener un diccionario de los parámetros del modelo y luego guardar ese diccionario utilizando el módulo `pickle` de Python. Luego, cuando queramos cargar el modelo nuevamente, podemos utilizar el método `load_state_dict()` para cargar el diccionario guardado en una nueva instancia del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz73sOEfYqec"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k--MBAWaMWhs"
   },
   "source": [
    "Subimos el modelo al Hub de Hugging Face para que otros miembros de la comunidad tengan acceso a él y también tengamos una copia en la nube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOVQlRcIMaqv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcIHW1wvMa4X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3OgUovfMc7y"
   },
   "source": [
    "Creamos el repositorio donde guardaremos nuestro modelo en el Hub de Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpFW31wvMfZi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ92rXRsMfrS"
   },
   "source": [
    "Subimos nuestro checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p91-vRylMi-i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JI8MWR46MkcX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szGJg5vXMiqJ"
   },
   "source": [
    "Carguemos el checkpoint en un nuevo directorio llamado `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9EZR8N9MoiL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmt75rY9MqEs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q04Bj7E3Mp3F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3hPofnXMpiV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dX7GS5UlYDj"
   },
   "source": [
    "Ahora carguemos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uneG6LNMlZ89"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSzjcl6rMrmd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uPfKcCKMsVr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faFyEYk7Mss-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EodfLQpFMu7U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovd0VkXhMuwh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN-IqYBmMugu"
   },
   "outputs": [],
   "source": [
    "ejemplo_2 = \"Axolotls are members of the tiger salamander, or Ambystoma tigrinum, species complex, along with all other Mexican species of Ambystoma.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He-_-eWWlAn-"
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "En este módulo aprendimos a utilizar `torchtext` para entrenar un modelo de clasificación con datos reales. \n",
    "\n",
    "1. Empezamos por preprocesar los datos mediante la tokenización y la construcción de un vocabulario. \n",
    "\n",
    "2. Luego creamos un conjunto de datos de PyTorch y lo usamos para entrenar un modelo de clasificación con una arquitectura de red neuronal. \n",
    "\n",
    "3. Probamos el modelo con un conjunto de pruebas\n",
    "\n",
    "4. Luego realizamos inferencia en nuevos datos. \n",
    "\n",
    "5. Finalmente, guardamos nuestro modelo entrenado para que pueda ser utilizado más adelante para otras tareas."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
